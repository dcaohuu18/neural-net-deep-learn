{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** there is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.\n",
    "\n",
    "![binary-output-net](binary-output-net.png)\n",
    "\n",
    "**Solution:** \n",
    "* ``The weights that start from the 0th neuron in the old output layer: 6[0, 0, 0,-1]``\n",
    "* ``The weights that start from the 1st neuron in the old output layer: 6[0, 0, 0, 1]``\n",
    "* ``The weights that start from the 2nd neuron in the old output layer: 6[0, 0, 1, 0]``\n",
    "* ``The weights that start from the 3rd neuron in the old output layer: 6[0, 0, 1, 1]``\n",
    "* ``The weights that start from the 4th neuron in the old output layer: 6[0, 1, 0, 0]``\n",
    "* ``The weights that start from the 5th neuron in the old output layer: 6[0, 0, 1, 1]``\n",
    "* ``The weights that start from the 6th neuron in the old output layer: 6[0, 1, 1, 0]``\n",
    "* ``The weights that start from the 7th neuron in the old output layer: 6[0, 1, 1, 1]``\n",
    "* ``The weights that start from the 8th neuron in the old output layer: 6[1, 0, 0, 0]``\n",
    "* ``The weights that start from the 9th neuron in the old output layer: 6[1, 0, 0, 1]``\n",
    "\n",
    "(*Why 6? see the graph of the sigmoid (aka logistic) function*)\n",
    "\n",
    "The bias of all the neurons in the new output layer is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Prove the assertion: the choice of $\\Delta v$ which minimizes $\\nabla\tC \\cdot \\Delta v$ is $\\Delta v = -\\eta \\nabla C$, where $\\eta = \\epsilon \\big/ \\lVert \\nabla C \\rVert$ is determined by the size constraint $\\lVert \\Delta v \\rVert= \\epsilon$. *Hint: If you're not already familiar with the Cauchy-Schwarz inequality, you may find it helpful to familiarize yourself with it.*\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lvert \\Delta C \\rvert = \\lvert \\nabla C \\cdot \\Delta v \\rvert & \\leq \\lVert \\nabla C \\rVert \\lVert \\Delta v \\rVert \\;\\;\\; \\text{(Cauchy-Schwarz)}\n",
    "\\\\\n",
    "& \\leq \\epsilon \\lVert \\nabla C \\rVert\n",
    "\\\\\n",
    "& \\leq \\frac{\\epsilon \\lVert \\nabla C \\rVert^2}{\\lVert \\nabla C \\rVert}\n",
    "\\\\\n",
    "& \\leq \\frac{\\epsilon}{\\lVert \\nabla C \\rVert} \\times (\\nabla C \\cdot \\nabla C)\n",
    "\\\\\n",
    "& \\leq \\eta \\; (\\nabla C \\cdot \\nabla C) = (\\eta\\nabla C) \\cdot \\nabla C\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because we want to decrease $C$, we want $\\Delta C < 0$:\n",
    "$$\n",
    "\\nabla C \\cdot \\Delta v \\geq (-\\eta \\nabla C) \\cdot \\nabla C\n",
    "$$\n",
    "The equality occurs  at $\\Delta v = - \\eta \\nabla C$. In other words, $\\Delta C = \\nabla C \\cdot \\Delta v$ is minimized when $\\Delta v = -\\eta \\nabla C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What happens when $C$ is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?\n",
    "\n",
    "**Solution:**\n",
    "<div>\n",
    "    <img src=\"one-var-grad-descent-graph.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "In this case, $C = v^3 + 2v^2$. We have: $\\nabla C = \\frac{\\partial C}{\\partial v} = \\frac{dC}{dv} = 3v^2+4v$. At $v = -1$, $\\nabla C = 3\\times(-1)^2+4\\times(-1)=-1$. Let us pick $\\epsilon = 0.2$ then $\\eta = \\frac{\\epsilon}{\\lVert \\nabla C \\rVert} = \\frac{0.2}{\\sqrt{(-1)^2}}=0.2$. According to the Gradient Descent algorithm, the optimal $\\Delta v = -\\eta \\nabla C = (-0.2) \\times (-1) = 0.2$. This means that moving to the right by $0.2$ will do the most to immediately decrease $C$. The graph confirms this statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, $x$, we update our weights and biases according to the rules $w_k \\rightarrow w'_k=w_k-\\eta \\partial C_x/\\partial w_k$ and $b_l→b'_l=b_l−\\eta \\partial C_x/ \\partial b_l$. Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20.\n",
    "\n",
    "**Solution:**\n",
    "* Advantage: Maximizes the speedup of computing the estimate of the gradient $\\nabla C$.\n",
    "* Disadvantage: $\\nabla C_x$ of the one training input is likely to be a poorer estimatate of the true $\\nabla C$ compared with the estimate based on the mini-batch of 20 training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write out Equation(22): $a' = \\sigma(wa + b)$ in component form, and verify that it gives the same result as the rule (4): $\\frac{1}{1+exp(-\\Sigma_j w_jx_j - b)}$ for computing the output of a sigmoid neuron.\n",
    "\n",
    "**Solution:** Suppose there are $n$ neurons in the former layer and $m$ neurons in the latter layer:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma(wa + b) & =\n",
    "\\sigma \\Biggl(\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & ... & w_{1n}\\\\\n",
    "w_{21} & w_{22} & ... & w_{2n}\\\\\n",
    "...    & ...    & ... & ...\\\\\n",
    "w_{m1} & w_{m2} & ... & w_{mn}\n",
    "\\end{bmatrix}\n",
    "\\times \n",
    "\\begin{bmatrix}\n",
    "a_1\\\\\n",
    "a_2\\\\\n",
    "...\\\\\n",
    "a_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "...\\\\\n",
    "b_m\n",
    "\\end{bmatrix}\n",
    "\\Biggr)\n",
    "\\\\\n",
    "\\\\\n",
    "& =\n",
    "\\sigma \\Biggl(\n",
    "\\begin{bmatrix}\n",
    "a_{1}w_{11} + a_{2}w_{12} + ... + a_{n}w_{1n}\\\\\n",
    "a_{1}w_{21} + a_{2}w_{22} + ... + a_{n}w_{2n}\\\\\n",
    "... \\\\\n",
    "a_{1}w_{m1} + a_{2}w_{m2} + ... + a_{n}w_{mn}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "...\\\\\n",
    "b_m\n",
    "\\end{bmatrix}\n",
    "\\Biggr)\n",
    "\\\\\n",
    "\\\\\n",
    "& =\n",
    "\\sigma \\Biggl(\n",
    "\\begin{bmatrix}\n",
    "a_{1}w_{11} + a_{2}w_{12} + ... + a_{n}w_{1n} + b_1\\\\\n",
    "a_{1}w_{21} + a_{2}w_{22} + ... + a_{n}w_{2n} + b_2\\\\\n",
    "... \\\\\n",
    "a_{1}w_{m1} + a_{2}w_{m2} + ... + a_{n}w_{mn} + b_m\n",
    "\\end{bmatrix}\n",
    "\\Biggr)\n",
    "\\\\\n",
    "\\\\\n",
    "& =\n",
    "\\begin{bmatrix}\n",
    "\\sigma(a_{1}w_{11} + a_{2}w_{12} + ... + a_{n}w_{1n} + b_1)\\\\\n",
    "\\sigma(a_{1}w_{21} + a_{2}w_{22} + ... + a_{n}w_{2n} + b_2)\\\\\n",
    "... \\\\\n",
    "\\sigma(a_{1}w_{m1} + a_{2}w_{m2} + ... + a_{n}w_{mn} + b_m)\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\\\\n",
    "& =\n",
    "\\begin{bmatrix}\n",
    "\\large{\\frac{1}{1+exp(-\\Sigma_j^n w_{1j}a_{j} - b_1)}}\\\\\n",
    "\\large{\\frac{1}{1+exp(-\\Sigma_j^n w_{2j}a_{j} - b_2)}}\\\\\n",
    "... \\\\\n",
    "\\large{\\frac{1}{1+exp(-\\Sigma_j^n w_{mj}a_{j} - b_m)}}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try creating a network with just two layers - an input and an output layer, no hidden layer - with 784 and 10 neurons, respectively. Train the network using stochastic gradient descent. What classification accuracy can you achieve?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "585a938ec471c889bf0cce0aed741a99eaf47ca09c0fa8393793bc5bfe77ba11"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
